# 5e.tools RAGç³»ç»Ÿæœ€ç»ˆå¼€å‘æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

åŸºäºé˜¿é‡Œäº‘text-embedding-v4å’ŒLangChainæ„å»ºçš„5e.tools RAGç³»ç»Ÿï¼Œä¸ºAIèŠå¤©æœºå™¨äººæä¾›ä¸“ä¸šçš„DND5Eè§„åˆ™æŸ¥è¯¢èƒ½åŠ›ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **æ™ºèƒ½è¯­ä¹‰æœç´¢**ï¼šåŸºäºé˜¿é‡Œäº‘æœ€æ–°text-embedding-v4æ¨¡å‹
- **ç²¾ç¡®è§„åˆ™æŸ¥è¯¢**ï¼šç›´æ¥è®¿é—®5e.toolså®˜æ–¹æ•°æ®
- **æ— ç¼é›†æˆ**ï¼šä¸ç°æœ‰é€šä¹‰åƒé—®èŠå¤©ç³»ç»Ÿå®Œç¾èåˆ
- **é«˜æ€§èƒ½**ï¼š1024ç»´å‘é‡ï¼Œ8192 Tokenå¤„ç†èƒ½åŠ›
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šè¦†ç›–100+ä¸»æµè¯­ç§

## æŠ€æœ¯æ¶æ„

```
DNDé—®é¢˜ â†’ é—®é¢˜åˆ†æ â†’ text-embedding-v4å‘é‡åŒ– â†’ ChromaDBæ£€ç´¢ â†’ å¢å¼ºå›ç­”
            â†‘                                         â†“
    5e.toolsæ•°æ®åŒæ­¥ â†’ æ–‡æ¡£é¢„å¤„ç† â†’ å‘é‡åŒ–å­˜å‚¨ â†’ å®šæœŸæ›´æ–°
```

### æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **å‘é‡åŒ–æ¨¡å‹**ï¼šé˜¿é‡Œäº‘text-embedding-v4
- **RAGæ¡†æ¶**ï¼šLangChain 
- **å‘é‡æ•°æ®åº“**ï¼šChromaDB
- **æ•°æ®æº**ï¼š5etools-mirror-3 GitHubä»“åº“
- **AIæ¨¡å‹**ï¼šé˜¿é‡Œäº‘é€šä¹‰åƒé—®ï¼ˆç°æœ‰ï¼‰
- **åç«¯æ¡†æ¶**ï¼šFastAPIï¼ˆç°æœ‰ï¼‰

## å®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒé…ç½®

#### 1.1 ä¾èµ–å®‰è£…
```bash
# æ›´æ–°requirements.txt
pip install -r requirements_updated.txt
```

```txt
# requirements_updated.txt
# ç°æœ‰ä¾èµ–ä¿æŒä¸å˜
fastapi==0.104.1
uvicorn==0.24.0
dashscope>=1.20.0
pydantic==2.5.0
python-multipart==0.0.6
python-dotenv==1.0.0

# RAGç³»ç»Ÿæ–°å¢ä¾èµ–
langchain==0.1.0
langchain-community==0.0.10
chromadb==0.4.22
GitPython==3.1.40
schedule==1.2.0
```

#### 1.2 ç¯å¢ƒå˜é‡é…ç½®
```bash
# åœ¨.envæ–‡ä»¶ä¸­æ·»åŠ ï¼ˆå¤ç”¨ç°æœ‰DASHSCOPE_API_KEYï¼‰
EMBEDDING_MODEL=text-embedding-v4
EMBEDDING_DIMENSION=1024
FIVETOOLS_DATA_PATH=./5etools-data
VECTOR_DB_PATH=./chroma_db
```

### ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®è·å–ä¸å¤„ç†

#### 2.1 5e.toolsæ•°æ®åŒæ­¥
```python
# scripts/sync_5etools_data.py
import git
import json
import os
from pathlib import Path
from typing import Dict, List

class FiveToolsDataSync:
    def __init__(self, local_path: str = "./5etools-data"):
        self.local_path = Path(local_path)
        self.repo_url = "https://github.com/5etools-mirror-3/5etools-src.git"
        
    def sync_data(self) -> bool:
        """åŒæ­¥5e.toolsæ•°æ®ï¼ˆä»…dataç›®å½•ï¼‰"""
        try:
            if self.local_path.exists():
                repo = git.Repo(self.local_path)
                repo.remotes.origin.pull()
                print("âœ… 5e.toolsæ•°æ®æ›´æ–°å®Œæˆ")
            else:
                # ä»…å…‹éš†dataç›®å½•ä»¥èŠ‚çœç©ºé—´
                git.Repo.clone_from(
                    self.repo_url, 
                    self.local_path,
                    depth=1  # æµ…å…‹éš†
                )
                print("âœ… 5e.toolsæ•°æ®åŒæ­¥å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def get_data_files(self) -> Dict[str, Path]:
        """è·å–æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        data_dir = self.local_path / "data"
        data_files = {}
        
        if data_dir.exists():
            # é‡ç‚¹æ•°æ®æ–‡ä»¶
            priority_files = [
                'spells.json', 'bestiary.json', 'classes.json', 
                'races.json', 'items.json', 'feats.json',
                'backgrounds.json', 'rules.json'
            ]
            
            for file_name in priority_files:
                file_path = data_dir / file_name
                if file_path.exists():
                    data_files[file_name.stem] = file_path
                    
        return data_files
```

#### 2.2 æ•°æ®é¢„å¤„ç†å™¨
```python
# processors/data_processor.py
import json
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class ProcessedDocument:
    id: str
    title: str
    content: str
    category: str
    metadata: Dict[str, Any]

class FiveToolsProcessor:
    def __init__(self):
        self.processors = {
            'spells': self._process_spells,
            'bestiary': self._process_monsters,
            'classes': self._process_classes,
            'races': self._process_races,
            'items': self._process_items,
            'feats': self._process_feats,
            'backgrounds': self._process_backgrounds,
        }
    
    def process_all_data(self, data_files: Dict[str, Path]) -> List[ProcessedDocument]:
        """å¤„ç†æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        documents = []
        
        for file_name, file_path in data_files.items():
            if file_name in self.processors:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        processed = self.processors[file_name](data)
                        documents.extend(processed)
                        print(f"âœ… å¤„ç†å®Œæˆ: {file_name}, ç”Ÿæˆ{len(processed)}ä¸ªæ–‡æ¡£")
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥: {file_name}, é”™è¯¯: {e}")
        
        return documents
    
    def _process_spells(self, data: Dict) -> List[ProcessedDocument]:
        """å¤„ç†æ³•æœ¯æ•°æ®"""
        documents = []
        
        for spell in data.get('spell', []):
            # æ„å»ºä¸°å¯Œçš„å¯æœç´¢å†…å®¹
            content_parts = [
                f"æ³•æœ¯åç§°: {spell.get('name', '')}",
                f"ç­‰çº§: {spell.get('level', 0)}ç¯æ³•æœ¯",
                f"å­¦æ´¾: {spell.get('school', '')}",
            ]
            
            # æ–½æ³•ä¿¡æ¯
            if 'time' in spell:
                casting_time = self._format_time(spell['time'])
                content_parts.append(f"æ–½æ³•æ—¶é—´: {casting_time}")
            
            if 'range' in spell:
                spell_range = self._format_range(spell['range'])
                content_parts.append(f"è·ç¦»: {spell_range}")
                
            if 'components' in spell:
                components = self._format_components(spell['components'])
                content_parts.append(f"æˆåˆ†: {components}")
            
            if 'duration' in spell:
                duration = self._format_duration(spell['duration'])
                content_parts.append(f"æŒç»­æ—¶é—´: {duration}")
            
            # æ³•æœ¯æè¿°
            if 'entries' in spell:
                description = self._format_entries(spell['entries'])
                content_parts.append(f"æè¿°: {description}")
            
            doc = ProcessedDocument(
                id=f"spell_{spell.get('name', '').lower().replace(' ', '_')}",
                title=f"{spell.get('name', '')} ({spell.get('level', 0)}ç¯{spell.get('school', '')}æ³•æœ¯)",
                content="\n".join(content_parts),
                category='spell',
                metadata={
                    'level': spell.get('level', 0),
                    'school': spell.get('school', ''),
                    'source': spell.get('source', ''),
                    'classes': spell.get('classes', {}).get('fromClassList', [])
                }
            )
            documents.append(doc)
        
        return documents
    
    def _process_monsters(self, data: Dict) -> List[ProcessedDocument]:
        """å¤„ç†æ€ªç‰©æ•°æ®"""
        documents = []
        
        for monster in data.get('monster', []):
            content_parts = [
                f"æ€ªç‰©åç§°: {monster.get('name', '')}",
                f"ä½“å‹: {monster.get('size', '')}",
                f"ç±»å‹: {monster.get('type', '')}",
                f"é˜µè¥: {monster.get('alignment', '')}",
                f"æŒ‘æˆ˜ç­‰çº§: {monster.get('cr', '')}",
            ]
            
            # å±æ€§å€¼
            if 'str' in monster:
                abilities = f"åŠ›é‡{monster.get('str', 10)} æ•æ·{monster.get('dex', 10)} ä½“è´¨{monster.get('con', 10)} æ™ºåŠ›{monster.get('int', 10)} æ„ŸçŸ¥{monster.get('wis', 10)} é­…åŠ›{monster.get('cha', 10)}"
                content_parts.append(f"å±æ€§: {abilities}")
            
            # æŠ€èƒ½å’Œç‰¹æ€§
            if 'skill' in monster:
                skills = ', '.join([f"{k}+{v}" for k, v in monster['skill'].items()])
                content_parts.append(f"æŠ€èƒ½: {skills}")
            
            if 'trait' in monster:
                traits = []
                for trait in monster['trait']:
                    traits.append(f"{trait.get('name', '')}: {self._format_entries(trait.get('entries', []))}")
                content_parts.append(f"ç‰¹æ€§: {'; '.join(traits)}")
            
            doc = ProcessedDocument(
                id=f"monster_{monster.get('name', '').lower().replace(' ', '_')}",
                title=f"{monster.get('name', '')} (CR {monster.get('cr', '')})",
                content="\n".join(content_parts),
                category='monster',
                metadata={
                    'cr': monster.get('cr', ''),
                    'type': monster.get('type', ''),
                    'size': monster.get('size', ''),
                    'source': monster.get('source', '')
                }
            )
            documents.append(doc)
        
        return documents
    
    # å…¶ä»–å¤„ç†æ–¹æ³•çš„ç®€åŒ–å®ç°...
    def _process_classes(self, data: Dict) -> List[ProcessedDocument]:
        """å¤„ç†èŒä¸šæ•°æ®ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        documents = []
        for cls in data.get('class', []):
            content = f"èŒä¸šåç§°: {cls.get('name', '')}\nç”Ÿå‘½éª°: {cls.get('hd', {}).get('faces', '')}d{cls.get('hd', {}).get('number', '')}"
            documents.append(ProcessedDocument(
                id=f"class_{cls.get('name', '').lower()}",
                title=cls.get('name', ''),
                content=content,
                category='class',
                metadata={'source': cls.get('source', '')}
            ))
        return documents
    
    def _format_entries(self, entries: List) -> str:
        """æ ¼å¼åŒ–æ¡ç›®å†…å®¹"""
        if not entries:
            return ""
        
        text_parts = []
        for entry in entries:
            if isinstance(entry, str):
                text_parts.append(entry)
            elif isinstance(entry, dict):
                if 'entries' in entry:
                    text_parts.extend(self._format_entries(entry['entries']))
                else:
                    text_parts.append(str(entry))
        
        return " ".join(text_parts)
    
    def _format_time(self, time_data: List) -> str:
        """æ ¼å¼åŒ–æ–½æ³•æ—¶é—´"""
        if not time_data:
            return ""
        time_item = time_data[0]
        return f"{time_item.get('number', 1)} {time_item.get('unit', '')}"
    
    def _format_range(self, range_data: Dict) -> str:
        """æ ¼å¼åŒ–è·ç¦»"""
        if range_data.get('type') == 'point':
            return f"{range_data.get('distance', {}).get('amount', '')} {range_data.get('distance', {}).get('type', '')}"
        return str(range_data.get('type', ''))
    
    def _format_components(self, comp_data: Dict) -> str:
        """æ ¼å¼åŒ–æˆåˆ†"""
        components = []
        if comp_data.get('v'): components.append('è¨€è¯­')
        if comp_data.get('s'): components.append('å§¿åŠ¿')
        if comp_data.get('m'): components.append('ææ–™')
        return ', '.join(components)
    
    def _format_duration(self, dur_data: List) -> str:
        """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
        if not dur_data:
            return ""
        dur_item = dur_data[0]
        if dur_item.get('type') == 'instant':
            return 'ç¬é—´'
        elif dur_item.get('type') == 'timed':
            return f"{dur_item.get('duration', {}).get('amount', '')} {dur_item.get('duration', {}).get('type', '')}"
        return str(dur_item.get('type', ''))
```

### ç¬¬ä¸‰é˜¶æ®µï¼šRAGç³»ç»Ÿæ ¸å¿ƒ

#### 3.1 å‘é‡å­˜å‚¨ç³»ç»Ÿ
```python
# rag/vector_store.py
import chromadb
from langchain_community.embeddings import DashScopeEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import List, Dict
from config import config

class FiveToolsVectorStore:
    def __init__(self, persist_directory: str = "./chroma_db"):
        # ä½¿ç”¨é˜¿é‡Œäº‘text-embedding-v4
        self.embeddings = DashScopeEmbeddings(
            model="text-embedding-v4",
            dashscope_api_key=config.DASHSCOPE_API_KEY,
            dimension=1024,  # ä½¿ç”¨1024ç»´åº¦å¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ
            output_type="dense"  # ä½¿ç”¨ç¨ å¯†å‘é‡
        )
        
        # åˆå§‹åŒ–ChromaDB
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection_name = "5etools_knowledge_v4"
        
        # æ–‡æœ¬åˆ†å‰²å™¨ - é’ˆå¯¹8192 Tokenä¼˜åŒ–
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,  # é€‚é…8192 Tokené™åˆ¶
            chunk_overlap=200,
            length_function=len,
        )
        
    def build_index(self, documents: List[ProcessedDocument]) -> bool:
        """æ„å»ºå‘é‡ç´¢å¼•"""
        try:
            # é‡å»ºé›†åˆ
            try:
                self.client.delete_collection(self.collection_name)
            except:
                pass
            
            collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            
            # æ‰¹é‡å¤„ç† - éµå¾ªAPIé™åˆ¶ï¼ˆæ¯æ‰¹æœ€å¤š10æ¡ï¼‰
            batch_size = 8  # ä¿å®ˆè®¾ç½®ï¼Œç¡®ä¿ç¨³å®šæ€§
            total_chunks = 0
            
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i+batch_size]
                
                # å‡†å¤‡æ‰¹é‡æ•°æ®
                texts = []
                metadatas = []
                ids = []
                
                for doc in batch_docs:
                    # åˆ†å‰²é•¿æ–‡æ¡£
                    chunks = self.text_splitter.split_text(doc.content)
                    
                    for j, chunk in enumerate(chunks):
                        chunk_id = f"{doc.id}_chunk_{j}"
                        texts.append(chunk)
                        metadatas.append({
                            'document_id': doc.id,
                            'title': doc.title,
                            'category': doc.category,
                            'chunk_index': j,
                            **doc.metadata
                        })
                        ids.append(chunk_id)
                
                if texts:
                    # ä½¿ç”¨text-embedding-v4æ‰¹é‡åµŒå…¥
                    embeddings = self.embeddings.embed_documents(texts)
                    
                    collection.add(
                        embeddings=embeddings,
                        documents=texts,
                        metadatas=metadatas,
                        ids=ids
                    )
                    
                    total_chunks += len(texts)
                    print(f"âœ… å·²å¤„ç† {i+len(batch_docs)}/{len(documents)} ä¸ªæ–‡æ¡£ï¼Œå…± {total_chunks} ä¸ªå‘é‡å—")
            
            print(f"ğŸ‰ å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼å…±ç”Ÿæˆ {total_chunks} ä¸ªå‘é‡å—")
            return True
            
        except Exception as e:
            print(f"âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return False
    
    def search(self, query: str, category_filter: str = None, n_results: int = 5) -> List[Dict]:
        """è¯­ä¹‰æœç´¢"""
        try:
            collection = self.client.get_collection(self.collection_name)
            
            # ä½¿ç”¨text-embedding-v4è¿›è¡ŒæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embeddings.embed_query(query)
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            where_filter = {}
            if category_filter:
                where_filter["category"] = category_filter
            
            # æ‰§è¡Œå‘é‡æœç´¢
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_filter if where_filter else None,
                include=["documents", "metadatas", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'score': 1 - results['distances'][0][i]  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
```

#### 3.2 RAGæŸ¥è¯¢å¼•æ“
```python
# rag/query_engine.py
from typing import List, Dict, Optional

class FiveToolsRAG:
    def __init__(self, vector_store: FiveToolsVectorStore):
        self.vector_store = vector_store
        
    async def query(self, 
                   question: str, 
                   category_hint: str = None,
                   max_context_length: int = 4000) -> Dict:
        """æ™ºèƒ½RAGæŸ¥è¯¢"""
        
        # 1. è¯­ä¹‰æœç´¢ç›¸å…³æ–‡æ¡£
        search_results = self.vector_store.search(
            query=question,
            category_filter=category_hint,
            n_results=10
        )
        
        # 2. è´¨é‡è¿‡æ»¤
        filtered_results = [r for r in search_results if r['score'] >= 0.6]
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(filtered_results, max_context_length)
        
        # 4. è¿”å›å¢å¼ºæŸ¥è¯¢ä¿¡æ¯
        return {
            'enhanced_query': question,
            'context': context,
            'sources': [r['metadata'] for r in filtered_results[:3]],
            'confidence': self._calculate_confidence(filtered_results),
            'found_results': len(filtered_results)
        }
    
    def _build_context(self, results: List[Dict], max_length: int) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not results:
            return ""
        
        context_parts = []
        current_length = 0
        
        for result in results:
            content = result['content']
            metadata = result['metadata']
            
            # æ·»åŠ æ¥æºæ ‡è¯†
            source_info = f"ã€{metadata['title']} - {metadata['category']}ã€‘"
            formatted_content = f"{source_info}\n{content}"
            
            if current_length + len(formatted_content) <= max_length:
                context_parts.append(formatted_content)
                current_length += len(formatted_content)
            else:
                break
        
        return "\n\n---\n\n".join(context_parts)
    
    def _calculate_confidence(self, results: List[Dict]) -> float:
        """è®¡ç®—æŸ¥è¯¢ç½®ä¿¡åº¦"""
        if not results:
            return 0.0
        
        # åŸºäºæœ€é«˜åˆ†æ•°å’Œç»“æœè´¨é‡
        best_score = results[0]['score']
        high_quality_count = len([r for r in results if r['score'] >= 0.8])
        
        confidence = best_score * (1 + min(high_quality_count / 5, 0.3))
        return min(confidence, 1.0)
```

### ç¬¬å››é˜¶æ®µï¼šç³»ç»Ÿé›†æˆ

#### 4.1 é›†æˆåˆ°ç°æœ‰èŠå¤©API
```python
# åœ¨backend/main.pyä¸­æ·»åŠ RAGåŠŸèƒ½

from rag.vector_store import FiveToolsVectorStore
from rag.query_engine import FiveToolsRAG
from processors.data_processor import FiveToolsProcessor
from scripts.sync_5etools_data import FiveToolsDataSync

# å…¨å±€RAGç³»ç»Ÿå®ä¾‹
rag_system = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    global rag_system
    
    try:
        print("ğŸš€ åˆå§‹åŒ–5e.tools RAGç³»ç»Ÿ...")
        
        vector_store = FiveToolsVectorStore()
        rag_system = FiveToolsRAG(vector_store)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ„å»ºç´¢å¼•
        try:
            collections = vector_store.client.list_collections()
            if not any(c.name == vector_store.collection_name for c in collections):
                print("ğŸ“š é¦–æ¬¡è¿è¡Œï¼Œå¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
                await build_vector_index()
        except:
            print("ğŸ“š æ„å»ºå‘é‡ç´¢å¼•...")
            await build_vector_index()
        
        print("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

async def build_vector_index():
    """æ„å»ºå‘é‡ç´¢å¼•"""
    # 1. åŒæ­¥æ•°æ®
    data_sync = FiveToolsDataSync()
    if not data_sync.sync_data():
        raise Exception("æ•°æ®åŒæ­¥å¤±è´¥")
    
    # 2. å¤„ç†æ•°æ®
    processor = FiveToolsProcessor()
    data_files = data_sync.get_data_files()
    documents = processor.process_all_data(data_files)
    
    # 3. æ„å»ºç´¢å¼•
    vector_store = FiveToolsVectorStore()
    if not vector_store.build_index(documents):
        raise Exception("å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """å¤„ç†èŠå¤©è¯·æ±‚ - é›†æˆRAGåŠŸèƒ½"""
    try:
        # ç°æœ‰éªŒè¯é€»è¾‘ä¿æŒä¸å˜...
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦RAGå¢å¼º
        needs_rag = await should_use_rag(request.message)
        
        enhanced_message = request.message
        rag_info = None
        
        if needs_rag and rag_system:
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            rag_result = await rag_system.query(
                question=request.message,
                category_hint=extract_category_hint(request.message)
            )
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨RAGç»“æœ
            if rag_result['confidence'] > 0.5 and rag_result['found_results'] > 0:
                enhanced_message = f"""{request.message}

[DND5Eè§„åˆ™å‚è€ƒèµ„æ–™]:
{rag_result['context']}

è¯·åŸºäºä»¥ä¸Šå®˜æ–¹è§„åˆ™èµ„æ–™å›ç­”é—®é¢˜ï¼Œå¦‚æœèµ„æ–™ä¸å®Œæ•´è¯·è¯´æ˜ã€‚"""
                rag_info = {
                    'used': True,
                    'confidence': rag_result['confidence'],
                    'sources': rag_result['sources']
                }
            else:
                rag_info = {'used': False, 'reason': 'æœªæ‰¾åˆ°ç›¸å…³è§„åˆ™èµ„æ–™'}
        
        # æ·»åŠ å¢å¼ºæ¶ˆæ¯åˆ°å†å²
        add_message_to_history(
            request.conversation_id, 
            "user", 
            enhanced_message, 
            request.user_id, 
            request.user_name
        )
        
        # è°ƒç”¨DashScope APIï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        history = get_conversation_history(request.conversation_id)
        
        from dashscope import Generation
        response = Generation.call(
            model=config.DASHSCOPE_MODEL,
            messages=history,
            max_tokens=1500,
            temperature=0.7,
            result_format='message'
        )
        
        if response.status_code == 200:
            ai_reply = response.output.choices[0].message.content
            
            # å¦‚æœä½¿ç”¨äº†RAGï¼Œæ·»åŠ æ¥æºä¿¡æ¯
            if rag_info and rag_info.get('used'):
                sources = rag_info['sources'][:2]  # æ˜¾ç¤ºå‰2ä¸ªæ¥æº
                source_text = ', '.join([s['title'] for s in sources])
                ai_reply += f"\n\nğŸ“š å‚è€ƒæ¥æº: {source_text}"
            
            # æ·»åŠ AIå›å¤åˆ°å†å²
            add_message_to_history(request.conversation_id, "assistant", ai_reply)
            
            return ChatResponse(reply=ai_reply, success=True)
        else:
            raise Exception(f"AIè°ƒç”¨å¤±è´¥: {response.message}")
        
    except Exception as e:
        logger.error(f"èŠå¤©å¤„ç†é”™è¯¯: {e}")
        return ChatResponse(reply="", success=False, error=str(e))

async def should_use_rag(message: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦RAGå¢å¼º"""
    dnd_keywords = [
        'æ³•æœ¯', 'spell', 'æ€ªç‰©', 'monster', 'èŒä¸š', 'class',
        'ç§æ—', 'race', 'ç‰©å“', 'item', 'è£…å¤‡', 'equipment',
        'ä¸“é•¿', 'feat', 'èƒŒæ™¯', 'background', 'è§„åˆ™', 'rule',
        'dnd', 'd&d', 'é¾™ä¸åœ°ä¸‹åŸ', 'å…ˆæ”»', 'è±å…', 'æ£€å®š'
    ]
    
    message_lower = message.lower()
    return any(keyword in message_lower for keyword in dnd_keywords)

def extract_category_hint(message: str) -> Optional[str]:
    """æå–ç±»åˆ«æç¤º"""
    category_keywords = {
        'spell': ['æ³•æœ¯', 'spell', 'å’’è¯­', 'é­”æ³•'],
        'monster': ['æ€ªç‰©', 'monster', 'æ•Œäºº', 'ç”Ÿç‰©'],
        'class': ['èŒä¸š', 'class', 'æˆ˜å£«', 'æ³•å¸ˆ', 'ç‰§å¸ˆ'],
        'race': ['ç§æ—', 'race', 'ç²¾çµ', 'çŸ®äºº', 'äººç±»'],
        'item': ['ç‰©å“', 'item', 'è£…å¤‡', 'equipment', 'æ­¦å™¨', 'æŠ¤ç”²'],
        'feat': ['ä¸“é•¿', 'feat', 'å¤©èµ‹'],
    }
    
    message_lower = message.lower()
    for category, keywords in category_keywords.items():
        if any(keyword in message_lower for keyword in keywords):
            return category
    
    return None
```

#### 4.2 æ·»åŠ RAGç®¡ç†å‘½ä»¤
```python
# åœ¨chatå‘½ä»¤ä¸­æ·»åŠ RAGç®¡ç†åŠŸèƒ½

case 'rag':
case 'RAG':
case 'çŸ¥è¯†åº“': {
    const arg2 = cmdArgs.getArgN(2) || '';
    
    switch (arg2) {
        case 'status':
        case 'çŠ¶æ€': {
            seal.replyToSender(ctx, msg, 'æ­£åœ¨æ£€æŸ¥RAGç³»ç»ŸçŠ¶æ€...');
            // è°ƒç”¨åç«¯çŠ¶æ€æ£€æŸ¥API
            (async () => {
                try {
                    const response = await fetch(`${CONFIG.API_BASE_URL}/rag/status`);
                    if (response.ok) {
                        const data = await response.json();
                        let statusMsg = `ğŸ¤– RAGçŸ¥è¯†åº“çŠ¶æ€:\n\n`;
                        statusMsg += `çŠ¶æ€: ${data.status}\n`;
                        statusMsg += `å‘é‡æ•°é‡: ${data.vector_count}\n`;
                        statusMsg += `æœ€åæ›´æ–°: ${data.last_update}\n`;
                        statusMsg += `æ¨¡å‹ç‰ˆæœ¬: text-embedding-v4\n`;
                        seal.replyToSender(ctx, msg, statusMsg);
                    } else {
                        seal.replyToSender(ctx, msg, 'âŒ æ— æ³•è·å–RAGç³»ç»ŸçŠ¶æ€');
                    }
                } catch (error) {
                    seal.replyToSender(ctx, msg, `âŒ æ£€æŸ¥çŠ¶æ€å¤±è´¥: ${error.message}`);
                }
            })();
            return seal.ext.newCmdExecuteResult(true);
        }
        
        case 'update':
        case 'æ›´æ–°': {
            const userPermission = getUserPermission(ctx);
            if (userPermission < 60) {
                seal.replyToSender(ctx, msg, 'âŒ æƒé™ä¸è¶³ï¼Œæ›´æ–°RAGçŸ¥è¯†åº“éœ€è¦60çº§æˆ–ä»¥ä¸Šæƒé™');
                return seal.ext.newCmdExecuteResult(true);
            }
            
            seal.replyToSender(ctx, msg, 'ğŸ”„ å¼€å§‹æ›´æ–°RAGçŸ¥è¯†åº“ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...');
            // è°ƒç”¨åç«¯æ›´æ–°API
            (async () => {
                try {
                    const response = await fetch(`${CONFIG.API_BASE_URL}/rag/update`, {method: 'POST'});
                    if (response.ok) {
                        const data = await response.json();
                        if (data.success) {
                            seal.replyToSender(ctx, msg, `âœ… RAGçŸ¥è¯†åº“æ›´æ–°å®Œæˆ!\nå¤„ç†æ–‡æ¡£: ${data.documents_processed}\nç”Ÿæˆå‘é‡: ${data.vectors_created}`);
                        } else {
                            seal.replyToSender(ctx, msg, `âŒ çŸ¥è¯†åº“æ›´æ–°å¤±è´¥: ${data.error}`);
                        }
                    } else {
                        seal.replyToSender(ctx, msg, 'âŒ æ›´æ–°è¯·æ±‚å¤±è´¥');
                    }
                } catch (error) {
                    seal.replyToSender(ctx, msg, `âŒ æ›´æ–°å¤±è´¥: ${error.message}`);
                }
            })();
            return seal.ext.newCmdExecuteResult(true);
        }
        
        default: {
            let helpMsg = `ğŸ§  RAGçŸ¥è¯†åº“ç®¡ç†\n\n`;
            helpMsg += `å¯ç”¨å‘½ä»¤:\n`;
            helpMsg += `â€¢ .chat rag status - æŸ¥çœ‹çŸ¥è¯†åº“çŠ¶æ€\n`;
            helpMsg += `â€¢ .chat rag update - æ›´æ–°çŸ¥è¯†åº“ï¼ˆéœ€è¦60çº§æƒé™ï¼‰\n\n`;
            helpMsg += `ğŸ’¡ RAGç³»ç»Ÿä¼šè‡ªåŠ¨ä¸ºDND5Eç›¸å…³é—®é¢˜æä¾›å®˜æ–¹è§„åˆ™æ”¯æŒ\n`;
            helpMsg += `ğŸ“š æ•°æ®æ¥æº: 5e.toolså®˜æ–¹æ•°æ®\n`;
            helpMsg += `ğŸ¤– å‘é‡æ¨¡å‹: é˜¿é‡Œäº‘text-embedding-v4`;
            
            seal.replyToSender(ctx, msg, helpMsg);
            return seal.ext.newCmdExecuteResult(true);
        }
    }
}
```

### ç¬¬äº”é˜¶æ®µï¼šéƒ¨ç½²ä¼˜åŒ–

#### 5.1 Dockeré…ç½®
```dockerfile
# Dockerfileä¼˜åŒ–ç‰ˆ
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements_updated.txt requirements.txt

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p /app/5etools-data /app/chroma_db /app/logs

# ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV FIVETOOLS_DATA_PATH=/app/5etools-data
ENV VECTOR_DB_PATH=/app/chroma_db

# æš´éœ²ç«¯å£
EXPOSE 1478

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:1478/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py"]
```

#### 5.2 æˆæœ¬ç›‘æ§å’Œä¼˜åŒ–
```python
# utils/cost_monitor.py
import json
from datetime import datetime
from typing import Dict

class RAGCostMonitor:
    def __init__(self):
        self.cost_log_file = "rag_costs.json"
        
    def log_embedding_usage(self, tokens_used: int, operation_type: str):
        """è®°å½•åµŒå…¥APIä½¿ç”¨æƒ…å†µ"""
        try:
            # text-embedding-v4å®šä»·: 0.0005å…ƒ/åƒToken
            cost = (tokens_used / 1000) * 0.0005
            
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'operation': operation_type,
                'tokens_used': tokens_used,
                'cost_yuan': cost,
                'model': 'text-embedding-v4'
            }
            
            # è¯»å–ç°æœ‰æ—¥å¿—
            try:
                with open(self.cost_log_file, 'r') as f:
                    logs = json.load(f)
            except:
                logs = []
            
            logs.append(log_entry)
            
            # å†™å…¥æ—¥å¿—
            with open(self.cost_log_file, 'w') as f:
                json.dump(logs, f, indent=2)
                
            print(f"ğŸ’° åµŒå…¥APIä½¿ç”¨: {tokens_used} tokens, æˆæœ¬: Â¥{cost:.4f}")
            
        except Exception as e:
            print(f"âŒ æˆæœ¬è®°å½•å¤±è´¥: {e}")
    
    def get_daily_summary(self) -> Dict:
        """è·å–æ¯æ—¥æˆæœ¬æ‘˜è¦"""
        try:
            with open(self.cost_log_file, 'r') as f:
                logs = json.load(f)
            
            today = datetime.now().date().isoformat()
            today_logs = [log for log in logs if log['timestamp'].startswith(today)]
            
            total_tokens = sum(log['tokens_used'] for log in today_logs)
            total_cost = sum(log['cost_yuan'] for log in today_logs)
            
            return {
                'date': today,
                'total_tokens': total_tokens,
                'total_cost_yuan': total_cost,
                'operations_count': len(today_logs),
                'free_quota_remaining': max(0, 1000000 - total_tokens)  # 100ä¸‡å…è´¹é¢åº¦
            }
            
        except:
            return {'error': 'æ— æ³•è·å–æˆæœ¬æ•°æ®'}
```

## é¢„æœŸæ•ˆæœä¸ä¼˜åŠ¿

### ğŸ¯ åŠŸèƒ½æ•ˆæœ
- **ç²¾ç¡®å›ç­”**ï¼šåŸºäºå®˜æ–¹5e.toolsæ•°æ®çš„å‡†ç¡®è§„åˆ™è§£é‡Š
- **æ™ºèƒ½ç†è§£**ï¼štext-embedding-v4çš„100+è¯­ç§æ”¯æŒå’Œä»£ç ç†è§£èƒ½åŠ›
- **å¿«é€Ÿå“åº”**ï¼šæœ¬åœ°ChromaDBæ¯«ç§’çº§æ£€ç´¢
- **æ¥æºå¯è¿½æº¯**ï¼šæ¯ä¸ªå›ç­”éƒ½æ ‡æ³¨å…·ä½“çš„è§„åˆ™æ¥æº

### ğŸ’° æˆæœ¬ä¼˜åŠ¿
- **å…è´¹é¢åº¦**ï¼š100ä¸‡Tokenå…è´¹é¢åº¦ï¼Œè¶³å¤Ÿè¿è¡Œæ•°æœˆ
- **ä½æˆæœ¬è¿è¡Œ**ï¼šæ¯åƒTokenä»…0.0005å…ƒ
- **é¢„ä¼°æœˆåº¦æˆæœ¬**ï¼šæ­£å¸¸ä½¿ç”¨<20å…ƒ/æœˆ

### ğŸ”§ æŠ€æœ¯ä¼˜åŠ¿
- **æœ€æ–°æŠ€æœ¯**ï¼štext-embedding-v4æœ€æ–°æ¨¡å‹
- **é«˜è´¨é‡å‘é‡**ï¼š1024ç»´å‘é‡å¹³è¡¡æ€§èƒ½å’Œæ•ˆæœ
- **æ— ç¼é›†æˆ**ï¼šå¤ç”¨ç°æœ‰é˜¿é‡Œäº‘é…ç½®
- **æ˜“äºç»´æŠ¤**ï¼šè‡ªåŠ¨åŒ–æ•°æ®åŒæ­¥å’Œæ›´æ–°

## æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡é˜¿é‡Œäº‘text-embedding-v4å’ŒLangChainæ„å»ºçš„RAGç³»ç»Ÿï¼Œå°†ä¸ºæ‚¨çš„AIèŠå¤©æœºå™¨äººæä¾›ä¸“ä¸šçº§çš„DND5Eè§„åˆ™æŸ¥è¯¢èƒ½åŠ›ã€‚ç³»ç»Ÿè®¾è®¡æ³¨é‡å®ç”¨æ€§ã€æˆæœ¬æ§åˆ¶å’Œç»´æŠ¤ä¾¿åˆ©æ€§ï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ç”Ÿäº§çº§è§£å†³æ–¹æ¡ˆã€‚

ğŸš€ **ç«‹å³å¼€å§‹å®æ–½ï¼Œè®©æ‚¨çš„AIåŠ©æ‰‹æˆä¸ºæœ€ä¸“ä¸šçš„DND5Eè§„åˆ™ä¸“å®¶ï¼** 